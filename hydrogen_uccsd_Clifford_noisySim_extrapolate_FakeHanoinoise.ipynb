{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64454bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "import numpy as np\n",
    "# import pennylane as qml\n",
    "from qiskit import Aer, transpile, execute\n",
    "from qiskit.circuit import QuantumCircuit\n",
    "from qiskit.quantum_info import random_clifford, Pauli, Statevector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=6, edgeitems=10, linewidth=150, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9011594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qiskit\n",
    "import itertools\n",
    "from qiskit import *\n",
    "from qiskit.quantum_info import Clifford, random_clifford\n",
    "from qiskit.synthesis import synth_clifford_full\n",
    "from qiskit.quantum_info import hellinger_fidelity as hf\n",
    "\n",
    "from utils.pauli_checks import ChecksFinder, add_pauli_checks, add_meas_pauli_checks, add_linear_meas_pauli_checks,  search_for_pauli_list\n",
    "from utils.pauli_checks import gen_initial_layout, gen_final_layout, complete_postprocess, filter_results\n",
    "\n",
    "from utils.utils import norm_dict, total_counts\n",
    "from utils.vqe_utils import evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f3d24",
   "metadata": {},
   "source": [
    "#### I. Calibrating $\\tilde{f}$ in the noisy Clifford channel using hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9475a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trials = 1600\n",
    "num_qubits = 4\n",
    "def calibration_circuit(Clifford):\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    \n",
    "    clifford_circuit = Clifford.to_circuit()\n",
    "    qc.compose(clifford_circuit, qubits=[0,1,2,3], inplace=True)\n",
    "    \n",
    "    qc.measure_all()\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f86a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_C_list = []\n",
    "for i in range(total_trials):\n",
    "    Clifford = random_clifford(4)\n",
    "    cali_C_list.append(Clifford)\n",
    "    \n",
    "cali_circs = []\n",
    "for i in range(total_trials):\n",
    "    circuit = calibration_circuit(cali_C_list[i])\n",
    "    cali_circs.append(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05442099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_coupling_map(coupling_map):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(coupling_map)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, with_labels=True, node_color='lightblue', node_size=700, \n",
    "            arrowsize=20, font_size=15, pos=nx.circular_layout(G))\n",
    "    plt.title(\"Quantum Hardware Topology\", size=15)\n",
    "    plt.savefig(\"Quantum_Hardware_Topology.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a90131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grid_coupling_map(coupling_map, grid_width):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(coupling_map)\n",
    "\n",
    "    # Calculate grid positions for a 2D grid layout\n",
    "    pos = {i: (i % grid_width, -(i // grid_width)) for i in range(G.number_of_nodes())}\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=700,\n",
    "            arrowsize=20, font_size=15)\n",
    "    plt.title(\"Quantum Hardware Grid Topology\", size=15)\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.savefig(\"Quantum_Hardware_Topology.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308976f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coupling_map(existing_map, num_qubits, num_checks):\n",
    "    # Copy the existing map to avoid modifying the original\n",
    "    updated_map = existing_map.copy()\n",
    "\n",
    "    # Add connections between computational qubits and ancilla qubits\n",
    "    for q in range(num_qubits):\n",
    "        for a in range(num_qubits, num_qubits + num_checks):\n",
    "            updated_map.append([q, a])\n",
    "            updated_map.append([a, q])  # If bidirectional connections are needed\n",
    "\n",
    "    return updated_map\n",
    "\n",
    "def apply_noise_to_new_connections(noise_model, num_qubits, num_checks, error_rate):\n",
    "    # Create a depolarizing error for two-qubit gates\n",
    "    error = depolarizing_error(error_rate, 2)\n",
    "\n",
    "    # Apply this error to new connections\n",
    "    for q in range(num_qubits):\n",
    "        for a in range(num_qubits, num_qubits + num_checks):\n",
    "            noise_model.add_quantum_error(error, ['cx'], [q, a]) \n",
    "            noise_model.add_quantum_error(error, ['cx'], [a, q])\n",
    "\n",
    "    return noise_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e535a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService, Session, Sampler, Options\n",
    "from qiskit.providers.fake_provider import *\n",
    "from qiskit_aer.noise import NoiseModel\n",
    "from qiskit.providers.aer.noise import depolarizing_error\n",
    "from itertools import combinations\n",
    "import qiskit.providers.aer.noise as noise\n",
    "\n",
    "\n",
    "# service = QiskitRuntimeService(channel=\"ibm_quantum\", instance=\"ibm-q-ornl/anl/chm185\")\n",
    "service = QiskitRuntimeService()\n",
    "\n",
    "# Make a noise model\n",
    "fake_backend = FakeHanoi()\n",
    "noise_model = NoiseModel.from_backend(fake_backend)\n",
    "\n",
    "# prob_1 = 0.003  # 1-qubit gate\n",
    "# prob_2 = 0.03   # 2-qubit gate\n",
    "\n",
    "# # Quantinuum\n",
    "# prob_1 = 2.945e-4\n",
    "# prob_2 = 1.377e-2\n",
    "\n",
    "# Depolarizing quantum errors\n",
    "# error_1 = noise.depolarizing_error(prob_1, 1)\n",
    "# error_2 = noise.depolarizing_error(prob_2, 2)\n",
    "# add the bitflit error.\n",
    "\n",
    "# # # Add errors to noise model\n",
    "# noise_model = noise.NoiseModel()\n",
    "# noise_model.add_all_qubit_quantum_error(error_1, ['u1', 'u2', 'u3', 'sx', 'x'])\n",
    "# noise_model.add_all_qubit_quantum_error(error_2, ['cx'])\n",
    "\n",
    "# # Measurement error\n",
    "# # Assuming a symmetric readout error where '0' is read as '1' and vice versa with a certain probability\n",
    "# meas_error = 2.65e-2\n",
    "# meas_error_matrix = [[1 - meas_error, meas_error], [meas_error, 1 - meas_error]]\n",
    "# meas_error = noise.ReadoutError(meas_error_matrix)\n",
    "# noise_model.add_all_qubit_readout_error(meas_error)\n",
    "\n",
    "# Custom topology for an 8-qubit system in a loop\n",
    "# custom_coupling_map = [[0, 1], [1, 0],\n",
    "#                        [1, 2], [2, 1],\n",
    "#                        [2, 3], [3, 2],\n",
    "#                        [3, 4], [4, 3],\n",
    "#                        [4, 5], [5, 4],\n",
    "#                        [5, 6], [6, 5],\n",
    "#                        [6, 7], [7, 6],\n",
    "#                        [7, 0], [0, 7]]  # Closing the loop \n",
    "\n",
    "# Custom topology for a 9-qubit system in a 3x3 grid\n",
    "# custom_coupling_map = [\n",
    "#     # Horizontal connections\n",
    "#     [0, 1], [1, 0], [1, 2], [2, 1],  # First row\n",
    "#     [3, 4], [4, 3], [4, 5], [5, 4],  # Second row\n",
    "#     [6, 7], [7, 6], [7, 8], [8, 7],  # Third row\n",
    "    \n",
    "#     # Vertical connections\n",
    "#     [0, 3], [3, 0], [3, 6], [6, 3],  # First column\n",
    "#     [1, 4], [4, 1], [4, 7], [7, 4],  # Second column\n",
    "#     [2, 5], [5, 2], [5, 8], [8, 5]   # Third column\n",
    "# ]\n",
    "\n",
    "# coupling_map = custom_coupling_map\n",
    "# coupling_map = fake_backend.configuration().coupling_map\n",
    "\n",
    "# Construct fully-connected checks\n",
    "# updated_coupling_map = update_coupling_map(coupling_map, num_qubits, num_checks=4)\n",
    "# noise_model = apply_noise_to_new_connections(noise_model, num_qubits, num_checks=4, error_rate=0.02)\n",
    "    \n",
    "options = Options(optimization_level=2, resilience_level=1) # choose the proper levels on hardware\n",
    "options.simulator = {\n",
    "    \"noise_model\": noise_model,\n",
    "    \"basis_gates\": fake_backend.configuration().basis_gates,\n",
    "    \"couplng_map\": fake_backend.configuration().coupling_map,\n",
    "    \"seed_simulator\": 42\n",
    "}\n",
    "#backend = service.get_backend(\"\") \n",
    "backend = \"ibmq_qasm_simulator\" # use the simulator for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91542400",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(noise_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f458b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coupling_map(coupling_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_coupling_map(updated_coupling_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179888d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(service, backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "    \n",
    "    # define physical qubits to be used in the layout arguement\n",
    "    job = sampler.run(cali_circs, shots=1024, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "    \n",
    "    result = job.result()\n",
    "    \n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cfb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_b_lists = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    cali_b_lists.append(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808959a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cali_b_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184376ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_b_lists[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b77339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrating_f(cali_b_lists, cali_C_list, num_qubits):\n",
    "    d = 2**num_qubits\n",
    "    num_snapshots = len(cali_C_list)\n",
    "    \n",
    "    f_tilde = 0.\n",
    "    for b_dict, clifford in zip(cali_b_lists, cali_C_list):\n",
    "        F = computing_F(b_dict, clifford, num_qubits)\n",
    "        f_tilde += np.real((d*F - 1) / (d - 1))\n",
    "    \n",
    "    return f_tilde / num_snapshots\n",
    "\n",
    "\n",
    "def computing_F(b_dict, clifford, num_qubits):\n",
    "    zero_state = state_reconstruction('0'*num_qubits)\n",
    "    U = clifford.to_matrix()\n",
    "    \n",
    "    F = 0. + 0.j\n",
    "    denom = 0.\n",
    "    for b_state in list(b_dict.keys()):\n",
    "        F += np.trace(zero_state @ U.T.conj() @ state_reconstruction(b_state) @ U) * b_dict.get(b_state)\n",
    "        denom += b_dict.get(b_state)\n",
    "    return F / denom\n",
    "\n",
    "\n",
    "def state_reconstruction(b_str: str):\n",
    "    '''\n",
    "    '''\n",
    "    zero_state = np.array([[1,0],[0,0]])\n",
    "    one_state = np.array([[0,0], [0,1]])\n",
    "    rho = [1]\n",
    "    for i in b_str:\n",
    "        state_i = zero_state if i=='0' else one_state\n",
    "        rho = np.kron(rho, state_i)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b070cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "f_tilde = calibrating_f(cali_b_lists, cali_C_list, num_qubits)\n",
    "print(f'The calibrated f_tilde is {f_tilde}; while the noiseless reference is {1/(2**num_qubits+1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20e54a",
   "metadata": {},
   "source": [
    "#### II. Perform the standard shadow experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22656fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ansatz circuit\n",
    "\n",
    "# num_checks = 4\n",
    "# num_qubits = 4\n",
    "# total_qubits = num_checks + num_qubits\n",
    "def hydrogen_trial_circuit(num_qubits):\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    # prepare the Hartree-Fock state\n",
    "    qc.x(0)\n",
    "    qc.x(1)\n",
    "    \n",
    "    qc.rx(np.pi/2, 0)\n",
    "    qc.h(1)\n",
    "    qc.h(2)\n",
    "    qc.h(3)\n",
    "    \n",
    "    qc.cx(0,1)\n",
    "    qc.cx(1,2)\n",
    "    qc.cx(2,3)\n",
    "    \n",
    "    qc.rz(1.0, 3)\n",
    "    \n",
    "    qc.cx(2,3)\n",
    "    qc.cx(1,2)\n",
    "    qc.cx(0,1)\n",
    "    \n",
    "    qc.rx(-np.pi/2, 0)\n",
    "    qc.h(1)\n",
    "    qc.h(2)\n",
    "    qc.h(3)\n",
    "    \n",
    "    return qc\n",
    "\n",
    "\n",
    "def hydrogen_shadow_circuit(Clifford, num_qubits):\n",
    "    qc = hydrogen_trial_circuit(num_qubits)\n",
    "    \n",
    "    clifford_circuit = Clifford.to_circuit()\n",
    "    qc.compose(clifford_circuit, qubits=[0,1,2,3], inplace=True)\n",
    "    \n",
    "    qc.measure_all()\n",
    "    return qc\n",
    "\n",
    "def hydrogen_shadow_PCS_circuit(Clifford, num_qubits, num_checks):\n",
    "    total_qubits = num_qubits + num_checks\n",
    "    qc = hydrogen_trial_circuit(total_qubits)\n",
    "\n",
    "    clif_qc = Clifford.to_circuit()\n",
    "    \n",
    "    characters = ['I', 'Z']\n",
    "    strings = [''.join(p) for p in itertools.product(characters, repeat=num_qubits)]\n",
    "    \n",
    "    test_finder = ChecksFinder(num_qubits, clif_qc)\n",
    "    p1_list = []\n",
    "    for string in strings:\n",
    "        string_list = list(string)\n",
    "        result = test_finder.find_checks_sym(pauli_group_elem = string_list)\n",
    "        #print(result.p1_str, result.p2_str)\n",
    "        p1_list.append([result.p1_str, result.p2_str])\n",
    "        \n",
    "    sorted_list = sorted(p1_list, key=lambda s: s[1].count('I'))\n",
    "    pauli_list = sorted_list[-num_qubits -1:-1]\n",
    "    \n",
    "    #\n",
    "    initial_layout = {}\n",
    "    for i in range(0, num_qubits):\n",
    "        initial_layout[i] = [i]\n",
    "\n",
    "    final_layout = {}\n",
    "    for i in range(0, num_qubits):\n",
    "        final_layout[i] = [i]\n",
    "        \n",
    "    #add pauli check on two sides:\n",
    "    #specify the left and right pauli strings\n",
    "    pcs_qc_list = []\n",
    "    sign_list = []\n",
    "    pl_list = []\n",
    "    pr_list = []\n",
    "\n",
    "    for i in range(0, num_checks):\n",
    "        pl = pauli_list[i][0][2:]\n",
    "        pr = pauli_list[i][1][2:]\n",
    "        if i == 0:\n",
    "            temp_qc = add_pauli_checks(clif_qc, pl, pr, initial_layout, final_layout, False, False, False, False, False)\n",
    "            save_qc = add_pauli_checks(clif_qc, pl, pr, initial_layout, final_layout, False, False, False, False, False)\n",
    "            prev_qc = temp_qc\n",
    "        else:\n",
    "            temp_qc = add_pauli_checks(prev_qc, pl, pr, initial_layout, final_layout, False, False, False, False, False)\n",
    "            save_qc = add_pauli_checks(prev_qc, pl, pr, initial_layout, final_layout, False, False, False, False, False) \n",
    "            prev_qc = temp_qc\n",
    "        pl_list.append(pl)\n",
    "        pr_list.append(pr)\n",
    "        sign_list.append(pauli_list[i][0][:2])\n",
    "        pcs_qc_list.append(save_qc)\n",
    "\n",
    "    \n",
    "    qc.compose(pcs_qc_list[-1], qubits=[i for i in range(0, total_qubits)], inplace=True)\n",
    "    \n",
    "    qc.measure_all()\n",
    "    return sign_list, qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85968a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 4\n",
    "num_checks = 4\n",
    "C_list = []\n",
    "for i in range(total_trials):\n",
    "    Clifford = random_clifford(4)\n",
    "    C_list.append(Clifford)\n",
    "circs_list = []\n",
    "signs_list = []\n",
    "for check_id in range(1, num_checks + 1):\n",
    "    circs = []\n",
    "    signs = []\n",
    "    for i in range(total_trials):\n",
    "        sign, circuit = hydrogen_shadow_PCS_circuit(C_list[i], num_qubits, check_id)\n",
    "        signs.append(sign)\n",
    "        circs.append(circuit)\n",
    "    circs_list.append(circs)\n",
    "    signs_list.append(signs)\n",
    "    \n",
    "orign_circs = []\n",
    "for i in range(total_trials):\n",
    "    circuit = hydrogen_shadow_circuit(C_list[i], num_qubits)\n",
    "    orign_circs.append(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "circs_list[3][-1].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def runtime_check_experiment(service, backend, circuits):\n",
    "#     session = Session(service, backend=backend)\n",
    "#     sampler = Sampler(session=session, options=options)\n",
    "\n",
    "#     # same as the calibration process\n",
    "#     job = sampler.run(circs, shots=1024, initial_layout=[])\n",
    "#     print(f\"Job ID: {job.job_id()}\")\n",
    "#     print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "#     result = job.result()\n",
    "\n",
    "#     # Close the session only if all jobs are finished\n",
    "#     # and you don't need to run more in the session.\n",
    "#     session.close()\n",
    "        \n",
    "#     b_lists_check = []\n",
    "#     for i in range(total_trials):\n",
    "#         di = {}\n",
    "#         for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "#             di.update({key[:total_qubits]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "#         b_lists_check.append(di)\n",
    "        \n",
    "#     filtered_b_lists = []\n",
    "#     for i in range(total_trials):\n",
    "#         bit_list = ['1' if i == '+1' else '0' for i in signs_list[i][num_checks - 1::-1]]\n",
    "#     #     print(bit_list)\n",
    "#         filted_dist = filter_results_reindex(b_lists_check[i], total_qubits-num_checks, [j for j in range(0, num_checks)], bit_list)\n",
    "#         print(total_counts(filted_dist))\n",
    "#         filtered_b_lists.append(filted_dist)\n",
    "#     return filtered_b_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_results_reindex(dictionary, qubits, indexes, sign_list):\n",
    "    new_dict = {}\n",
    "    for key in dictionary.keys():\n",
    "        new_key = ''\n",
    "        for i in range(len(key)):\n",
    "            meas_index = i\n",
    "#             if i < len(sign_list):\n",
    "#                 print(key, \"index\", i, key[i], sign_list[meas_index])\n",
    "            if meas_index in indexes and key[i] == sign_list[meas_index]:\n",
    "                #the key equals the sign, keep\n",
    "                new_key = ''\n",
    "                break\n",
    "            if meas_index not in indexes:\n",
    "                new_key += key[i]\n",
    "        if new_key != '':\n",
    "            new_dict[new_key] = dictionary[key]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879146a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_lists_filtered = []\n",
    "check_id = 1\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(service, backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run(circs_list[check_id-1], shots=1024, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists_check.append(di)\n",
    "\n",
    "\n",
    "filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in signs_list[check_id-1][i][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    filted_dist = filter_results_reindex(b_lists_check[i], num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(total_counts(filted_dist))\n",
    "    filtered_b_lists.append(filted_dist)\n",
    "b_lists_filtered.append(filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_id = 2\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(service, backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run(circs_list[check_id-1], shots=1024, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists_check.append(di)\n",
    "\n",
    "\n",
    "filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in signs_list[check_id-1][i][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    filted_dist = filter_results_reindex(b_lists_check[i], num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(total_counts(filted_dist))\n",
    "    filtered_b_lists.append(filted_dist)\n",
    "b_lists_filtered.append(filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_id = 3\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(service, backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run(circs_list[check_id-1], shots=1024, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists_check.append(di)\n",
    "\n",
    "\n",
    "filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in signs_list[check_id-1][i][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    filted_dist = filter_results_reindex(b_lists_check[i], num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(total_counts(filted_dist))\n",
    "    filtered_b_lists.append(filted_dist)\n",
    "b_lists_filtered.append(filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check_id = 4\n",
    "# # Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "# with Session(service, backend=backend) as session:\n",
    "#     sampler = Sampler(session=session, options=options)\n",
    "\n",
    "#     # same as the calibration process\n",
    "#     job = sampler.run(circs_list[check_id-1], shots=1024, initial_layout=[])\n",
    "#     print(f\"Job ID: {job.job_id()}\")\n",
    "#     print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "#     result = job.result()\n",
    "\n",
    "#     # Close the session only if all jobs are finished\n",
    "#     # and you don't need to run more in the session.\n",
    "#     session.close()\n",
    "\n",
    "# b_lists_check = []\n",
    "\n",
    "# for i in range(total_trials):\n",
    "#     di = {}\n",
    "#     for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "#         di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "#     b_lists_check.append(di)\n",
    "\n",
    "\n",
    "# filtered_b_lists = []\n",
    "# for i in range(total_trials):\n",
    "#     bit_list = ['1' if i == '+1' else '0' for i in signs_list[check_id-1][i][check_id - 1::-1]]\n",
    "# #     print(bit_list)\n",
    "#     filted_dist = filter_results_reindex(b_lists_check[i], num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "#     print(total_counts(filted_dist))\n",
    "#     filtered_b_lists.append(filted_dist)\n",
    "# b_lists_filtered.append(filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3005fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(service, backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "    \n",
    "    # same as the calibration process\n",
    "    job = sampler.run(orign_circs, shots=1024, initial_layout=[i for i in range(0, num_qubits)])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "    \n",
    "    result = job.result()\n",
    "    \n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_lists = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + num_checks]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists.append(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4e6f2",
   "metadata": {},
   "source": [
    "Noiseless Experiments on qiskitruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef384db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService, Session, Sampler, Options\n",
    "\n",
    "options = Options(optimization_level=2, resilience_level=1)\n",
    "backend = service.get_backend(\"ibmq_qasm_simulator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(service, backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    job = sampler.run(orign_circs, shots=1024)\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "    \n",
    "    result = job.result()\n",
    "    \n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def filter_results(dictionary, qubits, indexes, sign_list):\n",
    "    new_dict = {}\n",
    "    for key in dictionary.keys():\n",
    "        new_key = ''\n",
    "        for i in range(len(key)):\n",
    "            meas_index = i\n",
    "            #print(key, meas_index, indexes)\n",
    "            if meas_index in indexes and key[i] == sign_list[meas_index]:\n",
    "                new_key = ''\n",
    "                break\n",
    "            if meas_index not in indexes:\n",
    "                new_key += key[i]\n",
    "        if new_key != '':\n",
    "            new_dict[new_key] = dictionary[key]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_lists_noiseless = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists_noiseless.append(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beedb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_expectation(b_lists, C_list, operator_list, num_qubits, f_tilde):\n",
    "#     \"\"\"\n",
    "#     Reconstruct a state approximation as an average over all snapshots in the shadow.\n",
    "#     Args:\n",
    "#         shadow (tuple): A shadow tuple obtained from `calculate_classical_shadow`.\n",
    "#         operator (np.ndarray):\n",
    "#         num_qubits\n",
    "#     Returns:\n",
    "#         Numpy array with the reconstructed quantum state.\n",
    "#     \"\"\"\n",
    "#     num_snapshots = len(b_lists)\n",
    "    \n",
    "#     # Averaging over snapshot states.\n",
    "#     expectation_list = np.zeros(len(operator_list))\n",
    "#     for i in range(num_snapshots):\n",
    "#         expectation_list += expectation_snapshot(b_lists[i], C_list[i], operator_list, num_qubits, f_tilde)\n",
    "        \n",
    "#     expectation_list /= num_snapshots\n",
    "#     return expectation_list\n",
    "\n",
    "\n",
    "# def expectation_snapshot(b_dict, clifford, operator_list, num_qubits, f_tilde):\n",
    "#     \"\"\"\n",
    "#     Helper function for `shadow_state_reconstruction` that reconstructs the overlap estimate from\n",
    "#     a single snapshot in a shadow. Implements Eq. (S23) from https://arxiv.org/pdf/2106.16235.pdf\n",
    "#     Args:\n",
    "#         b_dict (dict): The list of classical outcomes for the snapshot.\n",
    "#         clifford: Indices for the applied Pauli measurement.\n",
    "#         operator:\n",
    "#         num_qubits:\n",
    "#     Returns:\n",
    "#         Numpy array with the reconstructed snapshot.\n",
    "#     \"\"\"\n",
    "#     # reconstructing the snapshot state from random Clifford measurements\n",
    "#     U = clifford.to_matrix()\n",
    "#     I = np.eye(2**num_qubits)\n",
    "    \n",
    "#     # applying Eq. (S32), note that this expression is built upon random Clifford, so that inverting\n",
    "#     # the quantum channel follows Eq. (S29).\n",
    "#     snapshot_list = np.zeros(len(operator_list))\n",
    "#     denom = 0\n",
    "#     for b_state in list(b_dict.keys()):\n",
    "#         interm = 1 / f_tilde * U.conj().T @ state_reconstruction(b_state) @ U\n",
    "#         interm -= (1 / f_tilde - 1)/2**num_qubits * I\n",
    "        \n",
    "#         for index, operator in enumerate(operator_list):\n",
    "#             operator_matrix = operator.to_matrix()\n",
    "#             snapshot_list[index] += np.real(np.trace(operator_matrix @ interm) * b_dict.get(b_state))\n",
    "            \n",
    "#         denom += b_dict.get(b_state)\n",
    "    \n",
    "#     return snapshot_list / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_expectation(b_lists, b_lists_checks, b_lists_noiseless, C_list, operator_list, num_qubits, f_tilde):\n",
    "    \"\"\"\n",
    "    Reconstruct a state approximation as an average over all snapshots in the shadow.\n",
    "    Args:\n",
    "        shadow (tuple): A shadow tuple obtained from `calculate_classical_shadow`.\n",
    "        operator (np.ndarray):\n",
    "        num_qubits\n",
    "    Returns:\n",
    "        Numpy array with the reconstructed quantum state.\n",
    "    \"\"\"\n",
    "    num_snapshots = len(b_lists)\n",
    "    \n",
    "    # Averaging over snapshot states.\n",
    "    expectation_list = np.zeros(len(operator_list))\n",
    "    expectation_list_r = np.zeros(len(operator_list))\n",
    "    expectation_list_checks = [np.zeros(len(operator_list)) for i in range(len(b_lists_checks))]\n",
    "    expectation_list_noiseless = np.zeros(len(operator_list))\n",
    "    \n",
    "    for i in range(num_snapshots):\n",
    "        noisy, robust = expectation_snapshot(b_lists[i], C_list[i], operator_list, num_qubits, f_tilde)\n",
    "        expectation_list += noisy\n",
    "        expectation_list_r += robust\n",
    "        \n",
    "        for j in range(len(b_lists_checks)):\n",
    "            check = expectation_snapshot_noiseless(b_lists_checks[j][i], C_list[i], operator_list, num_qubits)\n",
    "            expectation_list_checks[j] += check \n",
    "            \n",
    "        \n",
    "        noiseless = expectation_snapshot_noiseless(b_lists_noiseless[i], C_list[i], operator_list, num_qubits)\n",
    "        expectation_list_noiseless += noiseless\n",
    "        \n",
    "    expectation_list /= num_snapshots\n",
    "    expectation_list_r /= num_snapshots\n",
    "    for j in range(len(b_lists_checks)):\n",
    "        expectation_list_checks[j] /= num_snapshots\n",
    "    expectation_list_noiseless /= num_snapshots\n",
    "    \n",
    "    return expectation_list, expectation_list_r, expectation_list_checks, expectation_list_noiseless\n",
    "\n",
    "\n",
    "def expectation_snapshot(b_dict, clifford, operator_list, num_qubits, f_tilde):\n",
    "    \"\"\"\n",
    "    Helper function for `shadow_state_reconstruction` that reconstructs the overlap estimate from\n",
    "    a single snapshot in a shadow. Implements Eq. (S23) from https://arxiv.org/pdf/2106.16235.pdf\n",
    "    Args:\n",
    "        b_dict (dict): The list of classical outcomes for the snapshot.\n",
    "        clifford: Indices for the applied Pauli measurement.\n",
    "        operator:\n",
    "        num_qubits:\n",
    "    Returns:\n",
    "        Numpy array with the reconstructed snapshot.\n",
    "    \"\"\"\n",
    "    f = 1/(2**num_qubits+1)\n",
    "    # reconstructing the snapshot state from random Clifford measurements\n",
    "    U = clifford.to_matrix()\n",
    "    I = np.eye(2**num_qubits)\n",
    "    \n",
    "    # applying Eq. (S32), note that this expression is built upon random Clifford, so that inverting\n",
    "    # the quantum channel follows Eq. (S29).\n",
    "    snapshot_list = np.zeros(len(operator_list))\n",
    "    snapshot_list_r = np.zeros(len(operator_list))\n",
    "    denom = 0\n",
    "    for b_state in list(b_dict.keys()):\n",
    "        matrix_part = U.conj().T @ state_reconstruction(b_state) @ U\n",
    "        \n",
    "        interm = 1 / f * matrix_part\n",
    "        interm -= (1 / f - 1)/2**num_qubits * I\n",
    "        \n",
    "        interm_r = 1 / f_tilde * matrix_part\n",
    "        interm_r -= (1 / f_tilde - 1) / 2**num_qubits * I\n",
    "        \n",
    "        for index, operator in enumerate(operator_list):\n",
    "            operator_matrix = operator.to_matrix()\n",
    "            snapshot_list[index] += np.real(np.trace(operator_matrix @ interm) * b_dict.get(b_state))\n",
    "            snapshot_list_r[index] += np.real(np.trace(operator_matrix @ interm_r) * b_dict.get(b_state))\n",
    "            \n",
    "        denom += b_dict.get(b_state)\n",
    "    \n",
    "    return snapshot_list / denom, snapshot_list_r / denom\n",
    "\n",
    "\n",
    "def expectation_snapshot_noiseless(b_dict, clifford, operator_list, num_qubits):\n",
    "    \"\"\"\n",
    "    Helper function for `shadow_state_reconstruction` that reconstructs the overlap estimate from\n",
    "    a single snapshot in a shadow. Implements Eq. (S23) from https://arxiv.org/pdf/2106.16235.pdf\n",
    "    Args:\n",
    "        b_dict (dict): The list of classical outcomes for the snapshot.\n",
    "        clifford: Indices for the applied Pauli measurement.\n",
    "        operator:\n",
    "        num_qubits:\n",
    "    Returns:\n",
    "        Numpy array with the reconstructed snapshot.\n",
    "    \"\"\"\n",
    "    f = 1/(2**num_qubits+1)\n",
    "    # reconstructing the snapshot state from random Clifford measurements\n",
    "    U = clifford.to_matrix()\n",
    "    I = np.eye(2**num_qubits)\n",
    "    \n",
    "    # applying Eq. (S32), note that this expression is built upon random Clifford, so that inverting\n",
    "    # the quantum channel follows Eq. (S29).\n",
    "    snapshot_list = np.zeros(len(operator_list))\n",
    "    denom = 0\n",
    "    for b_state in list(b_dict.keys()):\n",
    "        interm = 1/f * U.conj().T @ state_reconstruction(b_state) @ U\n",
    "        interm -= (1/f - 1)/2**num_qubits * I\n",
    "        \n",
    "        for index, operator in enumerate(operator_list):\n",
    "            operator_matrix = operator.to_matrix()\n",
    "            snapshot_list[index] += np.real(np.trace(operator_matrix @ interm) * b_dict.get(b_state))\n",
    "            \n",
    "        denom += b_dict.get(b_state)\n",
    "    \n",
    "    return snapshot_list / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbef319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the classical shadows postprocessing to get expectation values;\n",
    "\n",
    "Paulis = ['XXXX', 'YYYY', 'XYXY', 'YXYX', 'YYXX', 'XXYY', 'ZZZZ', 'ZZII', 'IIZZ']\n",
    "operator_list = []\n",
    "for pauli in Paulis:\n",
    "    operator_list.append(Pauli(pauli))\n",
    "\n",
    "psi = Statevector(hydrogen_trial_circuit(num_qubits))\n",
    "ref_list = []\n",
    "for operator in operator_list:\n",
    "    expect = np.array(psi).T.conj() @ operator.to_matrix() @ np.array(psi)\n",
    "    ref_list.append(expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_runs = 10\n",
    "shadow_range = [10, 40, 100, 400]#, 1000]#, 4000]\n",
    "expectation_shadow = np.zeros((len(shadow_range), len(Paulis), num_of_runs))\n",
    "expectation_shadow_r = np.zeros((len(shadow_range), len(Paulis), num_of_runs))\n",
    "expectation_shadow_check1 = np.zeros((len(shadow_range), len(Paulis), num_of_runs))\n",
    "expectation_shadow_check2 = np.zeros((len(shadow_range), len(Paulis), num_of_runs))\n",
    "expectation_shadow_check3 = np.zeros((len(shadow_range), len(Paulis), num_of_runs))\n",
    "expectation_shadow_check4 = np.zeros((len(shadow_range), len(Paulis), num_of_runs))\n",
    "expectation_shadow_noiseless = np.zeros((len(shadow_range), len(Paulis), num_of_runs))\n",
    "for i in range(num_of_runs):\n",
    "    for j, num_snapshots in enumerate(shadow_range):\n",
    "        indices = random.sample(range(total_trials), num_snapshots)\n",
    "        C_sublist = [C_list[k] for k in indices]\n",
    "        b_sublists = [b_lists[k] for k in indices]\n",
    "        b_sublists_check1 = [b_lists_filtered[0][k] for k in indices]\n",
    "        b_sublists_check2 = [b_lists_filtered[1][k] for k in indices]\n",
    "        b_sublists_check3 = [b_lists_filtered[2][k] for k in indices]\n",
    "        # b_sublists_check4 = [b_lists_filtered[3][k] for k in indices]\n",
    "        b_sublists_checks = [b_sublists_check1,b_sublists_check2, b_sublists_check3]#, b_sublists_check4]\n",
    "        b_sublists_noiseless = [b_lists_noiseless[k] for k in indices]\n",
    "        \n",
    "        expectation_list, expectation_list_r, expectation_list_checks, expectation_list_noiseless = compute_expectation(\n",
    "            b_sublists, b_sublists_checks,  b_sublists_noiseless, C_sublist, operator_list, 4, f_tilde\n",
    "        )\n",
    "        \n",
    "        expectation_shadow[j, :, i] = np.real(expectation_list)\n",
    "        expectation_shadow_r[j, :, i] = np.real(expectation_list_r)\n",
    "        expectation_shadow_check1[j, :, i] = np.real(expectation_list_checks[0])\n",
    "        expectation_shadow_check2[j, :, i] = np.real(expectation_list_checks[1])\n",
    "        expectation_shadow_check3[j, :, i] = np.real(expectation_list_checks[2])\n",
    "        # expectation_shadow_check4[j, :, i] = np.real(expectation_list_checks[3])\n",
    "        expectation_shadow_noiseless[j, :, i] = np.real(expectation_list_noiseless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cdf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error = np.zeros(len(shadow_range))\n",
    "# for i in range(len(shadow_range)):\n",
    "#     error[i] = np.mean([np.abs(np.median(expectation_shadow[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55022d0",
   "metadata": {},
   "source": [
    "#### Extrapolation of expectation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e4518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "medians = [np.median(check, axis=2) for check in [expectation_shadow_check1, expectation_shadow_check2, expectation_shadow_check3, expectation_shadow_check4]]\n",
    "\n",
    "shadow_size_index = -1  # largest shadow size\n",
    "pauli_index = 1  # Example observable index\n",
    "expectation_values = [median[shadow_size_index, pauli_index] for median in medians]\n",
    "\n",
    "# Fit a Straight Line\n",
    "check_numbers = [1, 2, 3, 4]  # Numeric x-values for fitting\n",
    "polynomial = Polynomial.fit(check_numbers, expectation_values, 1)\n",
    "\n",
    "# Extrapolate to the Fifth Layer\n",
    "extrapolated_check = 5\n",
    "extrapolated_value = polynomial(extrapolated_check)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(check_numbers, expectation_values, color='blue', label='Measured Data')\n",
    "plt.plot(np.linspace(1, 5, 400), polynomial(np.linspace(1, 5, 400)), color='red', label='Fitted Line')\n",
    "plt.scatter([extrapolated_check], [extrapolated_value], color='green', label='Extrapolated for 5th Layer')\n",
    "\n",
    "plt.xlabel('Number of Check Layers')\n",
    "plt.ylabel(f'Median Expectation Value for {shadow_range[shadow_size_index]} snapshots')\n",
    "plt.title(f'Extrapolation of Expectation Value for Oberservable {Paulis[pauli_index]}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381edba3",
   "metadata": {},
   "source": [
    "#### Calculate extrapolated checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44feccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = [np.median(check, axis=2) for check in [expectation_shadow_check1, expectation_shadow_check2, expectation_shadow_check3]]\n",
    "check_numbers = [1, 2, 3]  # Original check layers\n",
    "extrapolation_layers = [4]  # Layers 4 to 8 inclusive\n",
    "\n",
    "# Initialize a three-dimensional array to store extrapolated values\n",
    "# Dimensions: [extrapolated layers, shadow size, Paulis]\n",
    "expectation_check_limit = np.zeros((len(extrapolation_layers), len(shadow_range), len(Paulis)))\n",
    "\n",
    "for layer_index, layer in enumerate(extrapolation_layers):\n",
    "    for shadow_size_index in range(len(medians[0])):\n",
    "        for pauli_index in range(medians[0].shape[1]):\n",
    "            expectation_values = [median[shadow_size_index, pauli_index] for median in medians]\n",
    "            polynomial = Polynomial.fit(check_numbers, expectation_values, 1)\n",
    "            # Extrapolate the value for the current layer\n",
    "            extrapolated_value = polynomial(layer)\n",
    "            expectation_check_limit[layer_index, shadow_size_index, pauli_index] = extrapolated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Plotting for a specific Pauli index across all shadow sizes\n",
    "pauli_index = 1  \n",
    "shadow_size_index = -1 # largest shadow size\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plotting the extrapolated values for each extrapolated layer for the specific observable and shadow size\n",
    "for layer_index, layer in enumerate(extrapolation_layers):\n",
    "    plt.plot(shadow_range, expectation_check_limit[layer_index, :, pauli_index], marker='o', linestyle='-', label=f'Extrapolated for {layer}th Layer')\n",
    "\n",
    "plt.xlabel('Shadow Size')\n",
    "plt.ylabel(f'Extrapolated Median Expectation Value')\n",
    "plt.title(f'Extrapolated Values Across Shadow Sizes for Observable Index {pauli_index}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expectation_check_limit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.zeros(len(shadow_range))\n",
    "error_r = np.zeros(len(shadow_range))\n",
    "error_check1 = np.zeros(len(shadow_range))\n",
    "error_check2 = np.zeros(len(shadow_range))\n",
    "error_check3 = np.zeros(len(shadow_range))\n",
    "error_check4 = np.zeros(len(shadow_range))\n",
    "print(error_check4.shape)\n",
    "print(expectation_shadow_check4.shape)\n",
    "error_noiseless = np.zeros(len(shadow_range))\n",
    "\n",
    "for i in range(len(shadow_range)):\n",
    "    error[i] = np.mean([np.abs(np.median(expectation_shadow[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "    error_r[i] = np.mean([np.abs(np.median(expectation_shadow_r[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "    error_check1[i] = np.mean([np.abs(np.median(expectation_shadow_check1[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "    error_check2[i] = np.mean([np.abs(np.median(expectation_shadow_check2[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "    error_check3[i] = np.mean([np.abs(np.median(expectation_shadow_check3[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "    error_check4[i] = np.mean([np.abs(np.median(expectation_shadow_check4[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "    error_noiseless[i] = np.mean([np.abs(np.median(expectation_shadow_noiseless[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c67e2",
   "metadata": {},
   "source": [
    "#### Compute mean error for each extrapolated check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87493e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_check_limit = np.zeros((len(extrapolation_layers), len(shadow_range)))\n",
    "\n",
    "for layer_index, layer in enumerate(extrapolation_layers):\n",
    "    for shadow_size_index in range(len(shadow_range)):\n",
    "        # Calculate the mean error for this layer and shadow size across all Pauli indices\n",
    "        error_check_limit[layer_index, shadow_size_index] = np.mean(\n",
    "            [np.abs(expectation_check_limit[layer_index, shadow_size_index, pauli_index] - ref_list[pauli_index]) for pauli_index in range(len(Paulis))]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ba27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_check_limit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4), dpi=100)\n",
    "plt.plot(shadow_range, error, '--o', ms=8, color='tab:orange', label='noisy')\n",
    "plt.plot(shadow_range, error_check1, '--o', ms=8, color='tab:red', label='check1')\n",
    "plt.plot(shadow_range, error_check2, '--o', ms=8, color='tab:purple', label='check2')\n",
    "plt.plot(shadow_range, error_check3, '--o', ms=8, color='tab:olive', label='check3')\n",
    "# plt.plot(shadow_range, error_check4, '--o', ms=8, color='tab:pink', label='check4')\n",
    "plt.plot(shadow_range, error_r, '--^', ms=8, color='tab:green', label='robust')\n",
    "plt.plot(shadow_range, error_noiseless, '--x', ms=8, color='tab:blue', label='noiseless')\n",
    "\n",
    "# Plotting each layer of extrapolated checks\n",
    "colors = ['tab:brown', 'tab:gray', 'tab:cyan', 'tab:pink', 'tab:purple']  # Example colors for different layers\n",
    "for layer_index, layer in enumerate(extrapolation_layers):\n",
    "    plt.plot(shadow_range, error_check_limit[layer_index, :], '--o', ms=8, color=colors[layer_index % len(colors)], label=f'check {layer} (extrap)')\n",
    "\n",
    "# plt.legend(fontsize=14, loc='best')\n",
    "# plt.xlabel('Shadow size', fontsize=14)\n",
    "# plt.ylabel('Error', fontsize=14)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.tick_params(labelsize=14)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig('non_ideal_checks.png', dpi=100)\n",
    "# plt.show()\n",
    "\n",
    "# Adjust the legend to be outside without altering the figure size\n",
    "plt.legend(fontsize=10, loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "plt.xlabel('Shadow size', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "# Note: The figure's layout isn't altered with plt.tight_layout() in this case\n",
    "# Saving the figure with bbox_inches='tight' includes the external legend\n",
    "plt.savefig('non_idealchecks.png', dpi=100, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4), dpi=100)\n",
    "plt.plot(shadow_range, error, '--o', ms=8, color='tab:blue', label='Robust Shadow')\n",
    "plt.legend(fontsize=14, loc='best')\n",
    "plt.xlabel('Shadow size', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329155c1",
   "metadata": {},
   "source": [
    "#### III. Perform the Pauli Check experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8337b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pcs]",
   "language": "python",
   "name": "conda-env-pcs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
