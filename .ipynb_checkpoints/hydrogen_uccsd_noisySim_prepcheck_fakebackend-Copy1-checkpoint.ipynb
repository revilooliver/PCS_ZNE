{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64454bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "import numpy as np\n",
    "# import pennylane as qml\n",
    "# from qiskit import Aer, transpile, execute\n",
    "from qiskit.circuit import QuantumCircuit\n",
    "from qiskit.quantum_info import random_clifford, Pauli, Statevector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=6, edgeitems=10, linewidth=150, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9011594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qiskit\n",
    "import itertools\n",
    "from qiskit import *\n",
    "from qiskit.quantum_info import Clifford, random_clifford\n",
    "from qiskit.synthesis import synth_clifford_full\n",
    "from qiskit.quantum_info import hellinger_fidelity as hf\n",
    "\n",
    "from utils.pauli_checks import ChecksFinder, add_pauli_checks, add_meas_pauli_checks, add_linear_meas_pauli_checks,  search_for_pauli_list\n",
    "from utils.pauli_checks import gen_initial_layout, gen_final_layout, filter_results, pauli_strings_commute\n",
    "\n",
    "from utils.utils import norm_dict, total_counts\n",
    "# from utils.vqe_utils import evaluation\n",
    "from utils.postprocess import singlecheck_postprocess, rightchecks_postprocess, filter_results_reindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f3d24",
   "metadata": {},
   "source": [
    "#### I. Calibrating $\\tilde{f}$ in the noisy Clifford channel using hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9475a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trials = 10000\n",
    "num_qubits = 4\n",
    "def calibration_circuit(Clifford):\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    \n",
    "    clifford_circuit = Clifford.to_circuit()\n",
    "    qc.compose(clifford_circuit, qubits=[0,1,2,3], inplace=True)\n",
    "    \n",
    "    qc.measure_all()\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_C_list = []\n",
    "for i in range(total_trials):\n",
    "    Clifford = random_clifford(4)\n",
    "    cali_C_list.append(Clifford)\n",
    "    \n",
    "cali_circs = []\n",
    "for i in range(total_trials):\n",
    "    circuit = calibration_circuit(cali_C_list[i])\n",
    "    cali_circs.append(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e535a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qiskit_ibm_runtime import Session, Options, SamplerV2 as Sampler\n",
    "from qiskit_ibm_runtime import Session, Sampler, Options\n",
    "from qiskit_ibm_runtime.fake_provider import *\n",
    "from qiskit_aer import AerSimulator\n",
    "import qiskit_aer.noise as noise\n",
    "from itertools import combinations\n",
    "\n",
    "# Make a noise model\n",
    "fake_backend = FakeGeneva() # select backend\n",
    "noise_model = noise.NoiseModel.from_backend(fake_backend)\n",
    "\n",
    "options = Options(optimization_level=2, resilience_level=1) # choose the proper levels on hardware\n",
    "options.simulator = {\n",
    "    \"noise_model\": noise_model,\n",
    "    \"basis_gates\": fake_backend.configuration().basis_gates,\n",
    "    \"coupling_map\": fake_backend.configuration().coupling_map,\n",
    "    \"seed_simulator\": 42\n",
    "}\n",
    "\n",
    "# backend = service.get_backend(\"\") \n",
    "# backend = \"ibmq_qasm_simulator\" # use the simulator for now\n",
    "backend = AerSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179888d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # define physical qubits to be used in the layout arguement\n",
    "    job = sampler.run(cali_circs, shots=100)\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cfb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_b_lists = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    cali_b_lists.append(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b77339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrating_f(cali_b_lists, cali_C_list, num_qubits):\n",
    "    d = 2**num_qubits\n",
    "    num_snapshots = len(cali_C_list)\n",
    "    \n",
    "    f_tilde = 0.\n",
    "    for b_dict, clifford in zip(cali_b_lists, cali_C_list):\n",
    "        F = computing_F(b_dict, clifford, num_qubits)\n",
    "        f_tilde += np.real((d*F - 1) / (d - 1))\n",
    "    \n",
    "    return f_tilde / num_snapshots\n",
    "\n",
    "\n",
    "def computing_F(b_dict, clifford, num_qubits):\n",
    "    zero_state = state_reconstruction('0'*num_qubits)\n",
    "    U = clifford.to_matrix()\n",
    "    \n",
    "    F = 0. + 0.j\n",
    "    denom = 0.\n",
    "    for b_state in list(b_dict.keys()):\n",
    "        F += np.trace(zero_state @ U.T.conj() @ state_reconstruction(b_state) @ U) * b_dict.get(b_state)\n",
    "        denom += b_dict.get(b_state)\n",
    "    return F / denom\n",
    "\n",
    "\n",
    "def state_reconstruction(b_str: str):\n",
    "    '''\n",
    "    '''\n",
    "    zero_state = np.array([[1,0],[0,0]])\n",
    "    one_state = np.array([[0,0], [0,1]])\n",
    "    rho = [1]\n",
    "    for i in b_str:\n",
    "        state_i = zero_state if i=='0' else one_state\n",
    "        rho = np.kron(rho, state_i)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b070cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "f_tilde = calibrating_f(cali_b_lists, cali_C_list, num_qubits)\n",
    "print(f'The calibrated f_tilde is {f_tilde}; while the noiseless reference is {1/(2**num_qubits+1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20e54a",
   "metadata": {},
   "source": [
    "#### II. Perform the standard shadow experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22656fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ansatz circuit\n",
    "\n",
    "def hartree_fock_circuit(num_qubits):\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    # prepare the Hartree-Fock state\n",
    "    qc.x(0)\n",
    "    qc.x(1)\n",
    "    return qc\n",
    "\n",
    "#the circuit without hartree fock preperation\n",
    "def hydrogen_trial_circuit_noprep(num_qubits):\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "#     # prepare the Hartree-Fock state\n",
    "#     qc.x(0)\n",
    "#     qc.x(1)\n",
    "    \n",
    "    qc.rx(np.pi/2, 0)\n",
    "    qc.h(1)\n",
    "    qc.h(2)\n",
    "    qc.h(3)\n",
    "    \n",
    "    qc.cx(0,1)\n",
    "    qc.cx(1,2)\n",
    "    qc.cx(2,3)\n",
    "    \n",
    "    qc.rz(1.0, 3)\n",
    "    \n",
    "    qc.cx(2,3)\n",
    "    qc.cx(1,2)\n",
    "    qc.cx(0,1)\n",
    "    \n",
    "    qc.rx(-np.pi/2, 0)\n",
    "    qc.h(1)\n",
    "    qc.h(2)\n",
    "    qc.h(3)\n",
    "    \n",
    "    return qc\n",
    "\n",
    "def hydrogen_trial_circuit(num_qubits):\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    # prepare the Hartree-Fock state\n",
    "    qc.x(0)\n",
    "    qc.x(1)\n",
    "    \n",
    "    qc.rx(np.pi/2, 0)\n",
    "    qc.h(1)\n",
    "    qc.h(2)\n",
    "    qc.h(3)\n",
    "    \n",
    "    qc.cx(0,1)\n",
    "    qc.cx(1,2)\n",
    "    qc.cx(2,3)\n",
    "    \n",
    "    qc.rz(1.0, 3)\n",
    "    \n",
    "    qc.cx(2,3)\n",
    "    qc.cx(1,2)\n",
    "    qc.cx(0,1)\n",
    "    \n",
    "    qc.rx(-np.pi/2, 0)\n",
    "    qc.h(1)\n",
    "    qc.h(2)\n",
    "    qc.h(3)\n",
    "    \n",
    "    return qc\n",
    "\n",
    "\n",
    "def hydrogen_shadow_circuit(Clifford, num_qubits):\n",
    "    qc = hydrogen_trial_circuit(num_qubits)\n",
    "    \n",
    "    clifford_circuit = Clifford.to_circuit()\n",
    "    qc.compose(clifford_circuit, qubits=[0,1,2,3], inplace=True)\n",
    "    \n",
    "    qc.measure_all()\n",
    "    return qc\n",
    "\n",
    "def hydrogen_shadow_PCS_circuit(Clifford, num_qubits, num_checks, single_side = False):\n",
    "    total_qubits = num_qubits + num_checks\n",
    "    \n",
    "    qc = hydrogen_trial_circuit(total_qubits)\n",
    "\n",
    "    clif_qc = Clifford.to_circuit()\n",
    "    \n",
    "    characters = ['I', 'Z']\n",
    "    strings = [''.join(p) for p in itertools.product(characters, repeat=num_qubits)]\n",
    "    \n",
    "    test_finder = ChecksFinder(num_qubits, clif_qc)\n",
    "    p1_list = []\n",
    "    for string in strings:\n",
    "        string_list = list(string)\n",
    "        result = test_finder.find_checks_sym(pauli_group_elem = string_list)\n",
    "        #print(result.p1_str, result.p2_str)\n",
    "        p1_list.append([result.p1_str, result.p2_str])\n",
    "        \n",
    "    sorted_list = sorted(p1_list, key=lambda s: s[1].count('I'))\n",
    "    pauli_list = sorted_list[-num_qubits -1:-1]\n",
    "    \n",
    "    #\n",
    "    initial_layout = {}\n",
    "    for i in range(0, num_qubits):\n",
    "        initial_layout[i] = [i]\n",
    "\n",
    "    final_layout = {}\n",
    "    for i in range(0, num_qubits):\n",
    "        final_layout[i] = [i]\n",
    "        \n",
    "    #add pauli check on two sides:\n",
    "    #specify the left and right pauli strings\n",
    "    pcs_qc_list = []\n",
    "    sign_list = []\n",
    "    pl_list = []\n",
    "    pr_list = []\n",
    "\n",
    "    for i in range(0, num_checks):\n",
    "        pl = pauli_list[i][0][2:]\n",
    "        pr = pauli_list[i][1][2:]\n",
    "        if i == 0:\n",
    "            temp_qc = add_pauli_checks(clif_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, 0)\n",
    "            save_qc = add_pauli_checks(clif_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, 0)\n",
    "            prev_qc = temp_qc\n",
    "        else:\n",
    "            temp_qc = add_pauli_checks(prev_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, 0)\n",
    "            save_qc = add_pauli_checks(prev_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, 0) \n",
    "            prev_qc = temp_qc\n",
    "        pl_list.append(pl)\n",
    "        pr_list.append(pr)\n",
    "        sign_list.append(pauli_list[i][0][:2])\n",
    "        pcs_qc_list.append(save_qc)\n",
    "\n",
    "    \n",
    "    qc.compose(pcs_qc_list[-1], qubits=[i for i in range(0, total_qubits)], inplace=True)\n",
    "    \n",
    "    qc.measure_all()\n",
    "    return qc, (sign_list, pl_list, pr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hydrogen_shadow_PCS_checkprep(Clifford, num_qubits, num_checks, single_side=False):\n",
    "    total_qubits = num_qubits + num_checks\n",
    "    qc_prep = hartree_fock_circuit(total_qubits)\n",
    "    qc = hydrogen_trial_circuit_noprep(total_qubits)\n",
    "    clif_qc = Clifford.to_circuit()\n",
    "    \n",
    "    # Generate all combinations of 'I' and 'Z' for num_qubits\n",
    "    characters = ['I', 'Z']\n",
    "    strings = [''.join(p) for p in itertools.product(characters, repeat=num_qubits)]\n",
    "    \n",
    "    test_finder = ChecksFinder(num_qubits, clif_qc)\n",
    "    p1_list = []\n",
    "    for string in strings:\n",
    "        string_list = list(string)\n",
    "        result = test_finder.find_checks_sym(pauli_group_elem = string_list)\n",
    "        #print(result.p1_str, result.p2_str)\n",
    "        p1_list.append([result.p1_str, result.p2_str])\n",
    "        \n",
    "    sorted_list = sorted(p1_list, key=lambda s: s[1].count('I'))\n",
    "    pauli_list = sorted_list[-num_qubits -1:-1]\n",
    "    \n",
    "    initial_layout = {i: [i] for i in range(num_qubits)}\n",
    "    final_layout = initial_layout.copy()\n",
    "\n",
    "    #add pauli check on two sides:\n",
    "    #specify the left and right pauli strings\n",
    "    pcs_qc_list = []\n",
    "    sign_list = []\n",
    "    pl_list = []\n",
    "    pr_list = []\n",
    "            \n",
    "    commute_pls, commute_prs, commute_signs, anticommute_pls, anticommute_prs, anticommute_signs = classify_pauli_checks(pauli_list, num_checks, prep_str = 'XXXY')\n",
    "\n",
    "            \n",
    "    # first add the anticommute checks in the middle of the circuit\n",
    "    for j in range(0, len(anticommute_pls)):\n",
    "        pl = anticommute_pls[j]\n",
    "        pr = anticommute_prs[j]\n",
    "        sign = anticommute_signs[j]\n",
    "        if j == 0:\n",
    "            temp_qc = add_pauli_checks(clif_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, 0)\n",
    "            save_qc = add_pauli_checks(clif_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, 0)\n",
    "            prev_qc = temp_qc\n",
    "        else:\n",
    "            temp_qc = add_pauli_checks(prev_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, 0)\n",
    "            save_qc = add_pauli_checks(prev_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, 0) \n",
    "            prev_qc = temp_qc\n",
    "        pl_list.append(pl)\n",
    "        pr_list.append(pr)\n",
    "        sign_list.append(sign)\n",
    "        pcs_qc_list.append(save_qc)\n",
    "\n",
    "    if len(anticommute_pls) > 0:\n",
    "        qc.compose(pcs_qc_list[-1], qubits=[i for i in range(0, num_qubits + len(anticommute_pls))], inplace=True)\n",
    "    \n",
    "    # then add the commute checks at the beginning of the circuit\n",
    "    num_commute = len(commute_pls)\n",
    "    for k in range(0, len(commute_pls)):\n",
    "        pl = commute_pls[k]\n",
    "        pr = commute_prs[k]\n",
    "        sign = commute_signs[k]\n",
    "        if k == 0:\n",
    "            temp_qc = add_pauli_checks(qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, num_commute - k)\n",
    "            save_qc = add_pauli_checks(qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, num_commute - k)\n",
    "            prev_qc = temp_qc\n",
    "        else:\n",
    "            temp_qc = add_pauli_checks(prev_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, num_commute - k)\n",
    "            save_qc = add_pauli_checks(prev_qc, pl, pr, initial_layout, final_layout, False, single_side, False, False, False, num_commute - k) \n",
    "            prev_qc = temp_qc\n",
    "        pl_list.append(pl)\n",
    "        pr_list.append(pr)\n",
    "        sign_list.append(sign)\n",
    "        pcs_qc_list.append(save_qc)\n",
    "    \n",
    "    if len(commute_pls) > 0:\n",
    "        qc = pcs_qc_list[-1]\n",
    "        \n",
    "    if len(anticommute_pls) == 0:\n",
    "        qc.compose(clif_qc, inplace=True)\n",
    "\n",
    "    qc_prep.compose(qc, inplace=True)\n",
    "    qc_prep.measure_all()\n",
    "    return qc_prep, (sign_list, pl_list, pr_list)\n",
    "\n",
    "def classify_pauli_checks(pauli_list, num_checks, prep_str):\n",
    "    \"\"\"\n",
    "    Classifies Pauli checks into commuting and anti-commuting groups based on the preparation string.\n",
    "    \n",
    "    Parameters:\n",
    "    - pauli_list: List of Pauli checks\n",
    "    - num_checks: Number of checks to classify\n",
    "    - prep_str: Preparation string to determine commuting or anti-commuting\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of lists: (commute_pls, commute_prs, commute_signs, anticommute_pls, anticommute_prs, anticommute_signs)\n",
    "    \"\"\"\n",
    "    commute_pls, commute_prs, commute_signs = [], [], []\n",
    "    anticommute_pls, anticommute_prs, anticommute_signs = [], [], []\n",
    "    \n",
    "    for i in range(num_checks):\n",
    "        pl, pr = pauli_list[i][0][2:], pauli_list[i][1][2:]\n",
    "        sign = pauli_list[i][0][:2]\n",
    "        \n",
    "        if pauli_strings_commute(pl, prep_str):\n",
    "            commute_pls.append(pl)\n",
    "            commute_prs.append(pr)\n",
    "            commute_signs.append(sign)\n",
    "        else:\n",
    "            anticommute_pls.append(pl)\n",
    "            anticommute_prs.append(pr)\n",
    "            anticommute_signs.append(sign)\n",
    "    \n",
    "    return (commute_pls, commute_prs, commute_signs, anticommute_pls, anticommute_prs, anticommute_signs)\n",
    "\n",
    "    commute_pls, commute_prs, commute_signs, anticommute_pls, anticommute_prs, anticommute_signs = classify_pauli_checks(pauli_list, num_checks, 'XXXY')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85968a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 4\n",
    "num_checks = 4\n",
    "C_list = []\n",
    "for i in range(total_trials):\n",
    "    Clifford = random_clifford(4)\n",
    "    C_list.append(Clifford)\n",
    "circs_list = []\n",
    "checks_list = []\n",
    "for check_id in range(1, num_checks + 1):\n",
    "    circs = []\n",
    "    checks = []\n",
    "    for i in range(total_trials):\n",
    "        print(check_id, i)\n",
    "        circ, check = hydrogen_shadow_PCS_circuit(C_list[i], num_qubits, check_id, True)\n",
    "        circs.append(circ)\n",
    "        checks.append(check)\n",
    "    circs_list.append(circs)\n",
    "    checks_list.append(checks)\n",
    "    \n",
    "orign_circs = []\n",
    "for i in range(total_trials):\n",
    "    circuit = hydrogen_shadow_circuit(C_list[i], num_qubits)\n",
    "    orign_circs.append(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 4\n",
    "num_checks = 4\n",
    "\n",
    "prepcheck_circs_list = []\n",
    "prepchecks_list = []\n",
    "for check_id in range(1, num_checks + 1):\n",
    "    circs = []\n",
    "    checks = []\n",
    "    for i in range(total_trials):\n",
    "        print(check_id, i)\n",
    "        circ, check = hydrogen_shadow_PCS_checkprep(C_list[i], num_qubits, check_id, True)\n",
    "        circs.append(circ)\n",
    "        checks.append(check)\n",
    "    prepcheck_circs_list.append(circs)\n",
    "    prepchecks_list.append(checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_lists_filtered = []\n",
    "check_id = 1\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run(circs_list[check_id-1], shots=100, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists_check.append(di)\n",
    "\n",
    "\n",
    "filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in checks_list[check_id-1][i][0][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    output_dist = rightchecks_postprocess(b_lists_check[i], num_qubits, check_id, pr_list = checks_list[check_id - 1][i][2])\n",
    "    filted_dist = filter_results_reindex(output_dist, num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(total_counts(filted_dist))\n",
    "    filtered_b_lists.append(filted_dist)\n",
    "b_lists_filtered.append(filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfd173",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_lists_check[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dist = rightchecks_postprocess(b_lists_check[1], num_qubits, check_id, pr_list = checks_list[check_id - 1][1][2])\n",
    "output_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "circs_list[0][1].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_list = ['1' if i == '+1' else '0' for i in checks_list[check_id-1][1][0][check_id - 1::-1]]\n",
    "bit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1eef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filted_dist = filter_results_reindex(output_dist, num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "filted_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7aca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(circs_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5000cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prepcheck_circs_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fefcb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "circs_list[1][0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepcheck_circs_list[1][0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77fca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepcheck_circs_list[0][5].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd086b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_b_lists_filtered = []\n",
    "check_id = 1\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run( prepcheck_circs_list[check_id-1], shots=100, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "prep_b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    prep_b_lists_check.append(di)\n",
    "\n",
    "\n",
    "prep_filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in prepchecks_list[check_id-1][i][0][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    prep_output_dist = rightchecks_postprocess(prep_b_lists_check[i], num_qubits, check_id, pr_list = prepchecks_list[check_id - 1][i][2])\n",
    "    prep_filted_dist = filter_results_reindex(prep_output_dist, num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(i, total_counts(prep_filted_dist))\n",
    "    prep_filtered_b_lists.append(prep_filted_dist)\n",
    "prep_b_lists_filtered.append(prep_filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_id = 2\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run(circs_list[check_id-1], shots=100, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists_check.append(di)\n",
    "\n",
    "\n",
    "filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in checks_list[check_id-1][i][0][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    output_dist = rightchecks_postprocess(b_lists_check[i], num_qubits, check_id, pr_list = checks_list[check_id - 1][i][2])\n",
    "    filted_dist = filter_results_reindex(output_dist, num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(total_counts(filted_dist))\n",
    "    filtered_b_lists.append(filted_dist)\n",
    "b_lists_filtered.append(filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepcheck_circs_list[2-1][0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d518a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_id = 2\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run( prepcheck_circs_list[check_id-1], shots=100, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "prep_b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    prep_b_lists_check.append(di)\n",
    "\n",
    "\n",
    "prep_filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in prepchecks_list[check_id-1][i][0][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    prep_output_dist = rightchecks_postprocess(prep_b_lists_check[i], num_qubits, check_id, pr_list = prepchecks_list[check_id - 1][i][2])\n",
    "    prep_filted_dist = filter_results_reindex(prep_output_dist, num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(i, total_counts(prep_filted_dist))\n",
    "    prep_filtered_b_lists.append(prep_filted_dist)\n",
    "prep_b_lists_filtered.append(prep_filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_id = 3\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run(circs_list[check_id-1], shots=100, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists_check.append(di)\n",
    "\n",
    "\n",
    "filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in checks_list[check_id-1][i][0][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    output_dist = rightchecks_postprocess(b_lists_check[i], num_qubits, check_id, pr_list = checks_list[check_id - 1][i][2])\n",
    "    filted_dist = filter_results_reindex(output_dist, num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(total_counts(filted_dist))\n",
    "    filtered_b_lists.append(filted_dist)\n",
    "b_lists_filtered.append(filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_id = 3\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run( prepcheck_circs_list[check_id-1], shots=100, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "prep_b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    prep_b_lists_check.append(di)\n",
    "\n",
    "\n",
    "prep_filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in prepchecks_list[check_id-1][i][0][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    prep_output_dist = rightchecks_postprocess(prep_b_lists_check[i], num_qubits, check_id, pr_list = prepchecks_list[check_id - 1][i][2])\n",
    "    prep_filted_dist = filter_results_reindex(prep_output_dist, num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(i, total_counts(prep_filted_dist))\n",
    "    prep_filtered_b_lists.append(prep_filted_dist)\n",
    "prep_b_lists_filtered.append(prep_filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_id = 4\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run(circs_list[check_id-1], shots=100, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists_check.append(di)\n",
    "\n",
    "\n",
    "filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in checks_list[check_id-1][i][0][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    output_dist = rightchecks_postprocess(b_lists_check[i], num_qubits, check_id, pr_list = checks_list[check_id - 1][i][2])\n",
    "    filted_dist = filter_results_reindex(output_dist, num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(total_counts(filted_dist))\n",
    "    filtered_b_lists.append(filted_dist)\n",
    "b_lists_filtered.append(filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b_lists_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_id = 4\n",
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    # same as the calibration process\n",
    "    job = sampler.run( prepcheck_circs_list[check_id-1], shots=100, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "\n",
    "    result = job.result()\n",
    "\n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()\n",
    "\n",
    "prep_b_lists_check = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + check_id]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    prep_b_lists_check.append(di)\n",
    "\n",
    "\n",
    "prep_filtered_b_lists = []\n",
    "for i in range(total_trials):\n",
    "    bit_list = ['1' if i == '+1' else '0' for i in prepchecks_list[check_id-1][i][0][check_id - 1::-1]]\n",
    "#     print(bit_list)\n",
    "    prep_output_dist = rightchecks_postprocess(prep_b_lists_check[i], num_qubits, check_id, pr_list = prepchecks_list[check_id - 1][i][2])\n",
    "    prep_filted_dist = filter_results_reindex(prep_output_dist, num_qubits, [j for j in range(0, check_id)], bit_list)\n",
    "    print(i, total_counts(prep_filted_dist))\n",
    "    prep_filtered_b_lists.append(prep_filted_dist)\n",
    "prep_b_lists_filtered.append(prep_filtered_b_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3005fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit hardware jobs via Qiskit Runtime;\n",
    "\n",
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "    \n",
    "    # same as the calibration process\n",
    "    job = sampler.run(orign_circs, shots=100, initial_layout=[])\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "    \n",
    "    result = job.result()\n",
    "    \n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_lists = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits + num_checks]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists.append(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4e6f2",
   "metadata": {},
   "source": [
    "Noiseless Experiments on qiskitruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef384db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService, Session, Sampler, Options\n",
    "\n",
    "options = Options(optimization_level=2, resilience_level=1)\n",
    "# backend = service.get_backend(\"ibmq_qasm_simulator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(backend=backend) as session:\n",
    "    sampler = Sampler(session=session, options=options)\n",
    "\n",
    "    job = sampler.run(orign_circs, shots=100)\n",
    "    print(f\"Job ID: {job.job_id()}\")\n",
    "    print(f\">>> Job Status: {job.status()}\")\n",
    "    \n",
    "    result = job.result()\n",
    "    \n",
    "    # Close the session only if all jobs are finished\n",
    "    # and you don't need to run more in the session.\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_lists_noiseless = []\n",
    "\n",
    "for i in range(total_trials):\n",
    "    di = {}\n",
    "    for key in list(result.quasi_dists[i].binary_probabilities().keys()):\n",
    "        di.update({key[:num_qubits]: result.quasi_dists[i].binary_probabilities().get(key)})\n",
    "    b_lists_noiseless.append(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_expectation(b_lists, b_lists_checks, b_lists_noiseless, C_list, operator_list, num_qubits, f_tilde):\n",
    "    \"\"\"\n",
    "    Reconstruct a state approximation as an average over all snapshots in the shadow.\n",
    "    Args:\n",
    "        shadow (tuple): A shadow tuple obtained from `calculate_classical_shadow`.\n",
    "        operator (np.ndarray):\n",
    "        num_qubits\n",
    "    Returns:\n",
    "        Numpy array with the reconstructed quantum state.\n",
    "    \"\"\"\n",
    "    num_snapshots = len(b_lists)\n",
    "    \n",
    "    # Averaging over snapshot states.\n",
    "    expectation_list = np.zeros(len(operator_list))\n",
    "    expectation_list_r = np.zeros(len(operator_list))\n",
    "    expectation_list_checks = [np.zeros(len(operator_list)) for i in range(len(b_lists_checks))]\n",
    "    expectation_list_noiseless = np.zeros(len(operator_list))\n",
    "    \n",
    "    for i in range(num_snapshots):\n",
    "        noisy, robust = expectation_snapshot(b_lists[i], C_list[i], operator_list, num_qubits, f_tilde)\n",
    "        expectation_list += noisy\n",
    "        expectation_list_r += robust\n",
    "        \n",
    "        for j in range(len(b_lists_checks)):\n",
    "            check = expectation_snapshot_noiseless(b_lists_checks[j][i], C_list[i], operator_list, num_qubits)\n",
    "            expectation_list_checks[j] += check \n",
    "            \n",
    "        \n",
    "        noiseless = expectation_snapshot_noiseless(b_lists_noiseless[i], C_list[i], operator_list, num_qubits)\n",
    "        expectation_list_noiseless += noiseless\n",
    "        \n",
    "    expectation_list /= num_snapshots\n",
    "    expectation_list_r /= num_snapshots\n",
    "    for j in range(len(b_lists_checks)):\n",
    "        expectation_list_checks[j] /= num_snapshots\n",
    "    expectation_list_noiseless /= num_snapshots\n",
    "    \n",
    "    return expectation_list, expectation_list_r, expectation_list_checks, expectation_list_noiseless\n",
    "\n",
    "\n",
    "def expectation_snapshot(b_dict, clifford, operator_list, num_qubits, f_tilde):\n",
    "    \"\"\"\n",
    "    Helper function for `shadow_state_reconstruction` that reconstructs the overlap estimate from\n",
    "    a single snapshot in a shadow. Implements Eq. (S23) from https://arxiv.org/pdf/2106.16235.pdf\n",
    "    Args:\n",
    "        b_dict (dict): The list of classical outcomes for the snapshot.\n",
    "        clifford: Indices for the applied Pauli measurement.\n",
    "        operator:\n",
    "        num_qubits:\n",
    "    Returns:\n",
    "        Numpy array with the reconstructed snapshot.\n",
    "    \"\"\"\n",
    "    f = 1/(2**num_qubits+1)\n",
    "    # reconstructing the snapshot state from random Clifford measurements\n",
    "    U = clifford.to_matrix()\n",
    "    I = np.eye(2**num_qubits)\n",
    "    \n",
    "    # applying Eq. (S32), note that this expression is built upon random Clifford, so that inverting\n",
    "    # the quantum channel follows Eq. (S29).\n",
    "    snapshot_list = np.zeros(len(operator_list))\n",
    "    snapshot_list_r = np.zeros(len(operator_list))\n",
    "    denom = 0\n",
    "    for b_state in list(b_dict.keys()):\n",
    "        matrix_part = U.conj().T @ state_reconstruction(b_state) @ U\n",
    "        \n",
    "        interm = 1 / f * matrix_part\n",
    "        interm -= (1 / f - 1)/2**num_qubits * I\n",
    "        \n",
    "        interm_r = 1 / f_tilde * matrix_part\n",
    "        interm_r -= (1 / f_tilde - 1) / 2**num_qubits * I\n",
    "        \n",
    "        for index, operator in enumerate(operator_list):\n",
    "            operator_matrix = operator.to_matrix()\n",
    "            snapshot_list[index] += np.real(np.trace(operator_matrix @ interm) * b_dict.get(b_state))\n",
    "            snapshot_list_r[index] += np.real(np.trace(operator_matrix @ interm_r) * b_dict.get(b_state))\n",
    "            \n",
    "        denom += b_dict.get(b_state)\n",
    "    \n",
    "    return snapshot_list / denom, snapshot_list_r / denom\n",
    "\n",
    "\n",
    "def expectation_snapshot_noiseless(b_dict, clifford, operator_list, num_qubits):\n",
    "    \"\"\"\n",
    "    Helper function for `shadow_state_reconstruction` that reconstructs the overlap estimate from\n",
    "    a single snapshot in a shadow. Implements Eq. (S23) from https://arxiv.org/pdf/2106.16235.pdf\n",
    "    Args:\n",
    "        b_dict (dict): The list of classical outcomes for the snapshot.\n",
    "        clifford: Indices for the applied Pauli measurement.\n",
    "        operator:\n",
    "        num_qubits:\n",
    "    Returns:\n",
    "        Numpy array with the reconstructed snapshot.\n",
    "    \"\"\"\n",
    "    f = 1/(2**num_qubits+1)\n",
    "    # reconstructing the snapshot state from random Clifford measurements\n",
    "    U = clifford.to_matrix()\n",
    "    I = np.eye(2**num_qubits)\n",
    "    \n",
    "    # applying Eq. (S32), note that this expression is built upon random Clifford, so that inverting\n",
    "    # the quantum channel follows Eq. (S29).\n",
    "    snapshot_list = np.zeros(len(operator_list))\n",
    "    denom = 0\n",
    "    for b_state in list(b_dict.keys()):\n",
    "        interm = 1/f * U.conj().T @ state_reconstruction(b_state) @ U\n",
    "        interm -= (1/f - 1)/2**num_qubits * I\n",
    "        \n",
    "        for index, operator in enumerate(operator_list):\n",
    "            operator_matrix = operator.to_matrix()\n",
    "            snapshot_list[index] += np.real(np.trace(operator_matrix @ interm) * b_dict.get(b_state))\n",
    "            \n",
    "        denom += b_dict.get(b_state)\n",
    "    \n",
    "    return snapshot_list / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbef319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the classical shadows postprocessing to get expectation values;\n",
    "\n",
    "Paulis = ['XXXX', 'YYYY', 'XYXY', 'YXYX', 'YYXX', 'XXYY', 'ZZZZ', 'ZZII', 'IIZZ',\n",
    "         'XZXZ', 'ZXZX', 'ZZXX', 'XXZZ', 'IIXX', 'XXII', 'XIIX']\n",
    "\n",
    "operator_list = []\n",
    "for pauli in Paulis:\n",
    "    operator_list.append(Pauli(pauli))\n",
    "\n",
    "psi = Statevector(hydrogen_trial_circuit(num_qubits))\n",
    "ref_list = []\n",
    "for operator in operator_list:\n",
    "    expect = np.array(psi).T.conj() @ operator.to_matrix() @ np.array(psi)\n",
    "    ref_list.append(expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_sublists_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbfb3d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_of_runs = 20\n",
    "shadow_range = [100, 400, 1000, 4000, 10000]#, 40000]\n",
    "num_of_checks = 4\n",
    "\n",
    "# Simplify initialization of expectation arrays\n",
    "expectations = {\n",
    "    name: np.zeros((len(shadow_range), len(Paulis), num_of_runs))\n",
    "    for name in [\"shadow\", \"shadow_r\", \"noiseless\"]\n",
    "    + [f\"check{k+1}\" for k in range(num_of_checks)]\n",
    "    + [f\"prepcheck{k+1}\" for k in range(num_of_checks)]\n",
    "}\n",
    "\n",
    "for j, num_snapshots in enumerate(shadow_range):\n",
    "    indices = random.sample(range(total_trials), num_snapshots)\n",
    "    print(f'num total snapshots = {num_snapshots}')\n",
    "\n",
    "    # Partition indices into 'num_of_runs' equally sized chunks\n",
    "    partitions = np.array_split(indices, num_of_runs)\n",
    "    # print('paritions', partitions)\n",
    "    # print(len(partitions))\n",
    "\n",
    "    for i, run_indices in enumerate(partitions):\n",
    "        print('i = ', i)\n",
    "        # Create sublists using list comprehensions\n",
    "        C_run_sublist = [C_list[idx] for idx in run_indices]\n",
    "        b_run_sublists_dict = {\n",
    "            \"base\": [b_lists[idx] for idx in run_indices],\n",
    "            \"noiseless\": [b_lists_noiseless[idx] for idx in run_indices],\n",
    "            \"checks\": [[b_lists_filtered[k][idx] for idx in run_indices] for k in range(num_of_checks)],\n",
    "            \"prepchecks\": [[prep_b_lists_filtered[k][idx] for idx in run_indices] for k in range(num_of_checks)],\n",
    "        }\n",
    "\n",
    "        # Compute expectations for each type of check\n",
    "        expectation_list, expectation_list_r, expectation_list_checks, expectation_list_noiseless = compute_expectation(\n",
    "            b_run_sublists_dict['base'], b_run_sublists_dict['checks'], b_run_sublists_dict['noiseless'], C_run_sublist, operator_list, 4, f_tilde\n",
    "        )\n",
    "        expectation_list, expectation_list_r, expectation_list_prepchecks, expectation_list_noiseless = compute_expectation(\n",
    "            b_run_sublists_dict['base'], b_run_sublists_dict['prepchecks'], b_run_sublists_dict['noiseless'], C_run_sublist, operator_list, 4, f_tilde\n",
    "        )\n",
    "\n",
    "        # Store the results\n",
    "        expectations['shadow'][j, :, i] = np.real(expectation_list)\n",
    "        expectations['shadow_r'][j, :, i] = np.real(expectation_list_r)\n",
    "        expectations['noiseless'][j, :, i] = np.real(expectation_list_noiseless)\n",
    "        for k in range(num_of_checks):\n",
    "            expectations[f\"check{k+1}\"][j, :, i] = np.real(expectation_list_checks[k])\n",
    "        for k in range(num_of_checks):\n",
    "            expectations[f\"prepcheck{k+1}\"][j, :, i] = np.real(expectation_list_prepchecks[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fa8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the expectations dictionary to a .npz file\n",
    "np.savez('expectations_fakeAlmaden.npz', **expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cd3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_of_runs = 20\n",
    "# shadow_range = [10, 50, 100, 500]#, 4000]\n",
    "# num_of_checks = 4\n",
    "# # Simplify initialization of expectation arrays\n",
    "# expectations = {\n",
    "#     name: np.zeros((len(shadow_range), len(Paulis), num_of_runs))\n",
    "#     for name in [\"shadow\", \"shadow_r\", \"noiseless\"]\n",
    "#     + [f\"check{k+1}\" for k in range(num_of_checks)]\n",
    "#     + [f\"prepcheck{k+1}\" for k in range(num_of_checks)]\n",
    "# }\n",
    "\n",
    "# for i in range(num_of_runs):\n",
    "#     for j, num_snapshots in enumerate(shadow_range):\n",
    "#         indices = random.sample(range(total_trials), num_snapshots)\n",
    "#         print(f'run # {i}, num snapshots = {num_snapshots}')\n",
    "        \n",
    "#         # Create sublists using list comprehensions\n",
    "#         C_sublist = [C_list[idx] for idx in indices]\n",
    "#         b_sublists_dict = {\n",
    "#             \"base\": [b_lists[idx] for idx in indices],\n",
    "#             \"noiseless\": [b_lists_noiseless[idx] for idx in indices],\n",
    "#             \"checks\": [[b_lists_filtered[k][idx] for idx in indices] for k in range(num_of_checks)],\n",
    "#             \"prepchecks\": [[prep_b_lists_filtered[k][idx] for idx in indices] for k in range(num_of_checks)],\n",
    "#         }\n",
    "        \n",
    "#         # Compute expectations for each type of check\n",
    "        \n",
    "#         expectation_list, expectation_list_r, expectation_list_checks, expectation_list_noiseless = compute_expectation(\n",
    "#             b_sublists_dict['base'], b_sublists_dict['checks'],  b_sublists_dict['noiseless'], C_sublist, operator_list, 4, f_tilde\n",
    "#         )\n",
    "#         expectation_list, expectation_list_r, expectation_list_prepchecks, expectation_list_noiseless = compute_expectation(\n",
    "#             b_sublists_dict['base'], b_sublists_dict['prepchecks'],  b_sublists_dict['noiseless'], C_sublist, operator_list, 4, f_tilde\n",
    "#         )\n",
    "    \n",
    "#         expectations['shadow'][j, :, i] = np.real(expectation_list)\n",
    "#         expectations['shadow_r'][j, :, i] = np.real(expectation_list_r)  \n",
    "#         expectations['noiseless'][j, :, i] = np.real(expectation_list_noiseless)  \n",
    "#         for k in range(num_of_checks):\n",
    "#             expectations[f\"check{k+1}\"][j, :, i] = np.real(expectation_list_checks[k])\n",
    "#         for k in range(num_of_checks):\n",
    "#             expectations[f\"prepcheck{k+1}\"][j, :, i] = np.real(expectation_list_prepchecks[k])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {\n",
    "    name: np.zeros(len(shadow_range))\n",
    "    for name in [\"shadow\", \"shadow_r\", \"noiseless\"]\n",
    "    + [f\"check{k+1}\" for k in range(num_of_checks)]\n",
    "    + [f\"prepcheck{k+1}\" for k in range(num_of_checks)]\n",
    "}\n",
    "for i in range(len(shadow_range)):\n",
    "    errors['shadow'][i] = np.mean([np.abs(np.median(expectations[\"shadow\"][i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "    errors['shadow_r'][i] = np.mean([np.abs(np.median(expectations[\"shadow_r\"][i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])  \n",
    "    errors['noiseless'][i] = np.mean([np.abs(np.median(expectations[\"noiseless\"][i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "    for k in range(num_of_checks):\n",
    "        errors[f\"check{k+1}\"][i] = np.mean([np.abs(np.median(expectations[f\"check{k+1}\"][i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "    for k in range(num_of_checks):\n",
    "        errors[f\"prepcheck{k+1}\"][i] = np.mean([np.abs(np.median(expectations[f\"prepcheck{k+1}\"][i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error = np.zeros(len(shadow_range))\n",
    "# error_r = np.zeros(len(shadow_range))\n",
    "# error_check1 = np.zeros(len(shadow_range))\n",
    "# error_check2 = np.zeros(len(shadow_range))\n",
    "# error_check3 = np.zeros(len(shadow_range))\n",
    "# error_check4 = np.zeros(len(shadow_range))\n",
    "# error_prepcheck1 = np.zeros(len(shadow_range))\n",
    "# error_prepcheck2 = np.zeros(len(shadow_range))\n",
    "# error_prepcheck3 = np.zeros(len(shadow_range))\n",
    "# error_prepcheck4 = np.zeros(len(shadow_range))\n",
    "# error_noiseless = np.zeros(len(shadow_range))\n",
    "\n",
    "# for i in range(len(shadow_range)):\n",
    "#     error[i] = np.mean([np.abs(np.median(expectation_shadow[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "#     error_r[i] = np.mean([np.abs(np.median(expectation_shadow_r[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "#     error_check1[i] = np.mean([np.abs(np.median(expectation_shadow_check1[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "#     error_check2[i] = np.mean([np.abs(np.median(expectation_shadow_check2[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "#     error_check3[i] = np.mean([np.abs(np.median(expectation_shadow_check3[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "#     error_check4[i] = np.mean([np.abs(np.median(expectation_shadow_check4[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "#     error_prepcheck1[i] = np.mean([np.abs(np.median(expectation_shadow_prepcheck1[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "#     error_prepcheck2[i] = np.mean([np.abs(np.median(expectation_shadow_prepcheck2[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "#     error_prepcheck3[i] = np.mean([np.abs(np.median(expectation_shadow_prepcheck3[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "#     error_prepcheck4[i] = np.mean([np.abs(np.median(expectation_shadow_prepcheck4[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])\n",
    "\n",
    "#     error_noiseless[i] = np.mean([np.abs(np.median(expectation_shadow_noiseless[i], axis=1)[j] - ref_list[j]) for j in range(len(ref_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a43ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_check1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4), dpi=100)\n",
    "plt.plot(shadow_range, errors['shadow'], '--o', ms=8, color='tab:orange', label='noisy')\n",
    "plt.plot(shadow_range, errors['shadow_r'], '--^', ms=8, color='tab:green', label='robust')\n",
    "plt.plot(shadow_range, errors['noiseless'], '--x', ms=8, color='tab:blue', label='noiseless')\n",
    "plt.plot(shadow_range, errors['check1'], '--o', ms=8, color='tab:red', label='check1')\n",
    "plt.plot(shadow_range, errors['check2'], '--o', ms=8, color='tab:purple', label='check2')\n",
    "plt.plot(shadow_range, errors['check3'], '--o', ms=8, color='tab:olive', label='check3')\n",
    "plt.plot(shadow_range, errors['check4'], '--o', ms=8, color='tab:pink', label='check4')\n",
    "plt.legend(fontsize=14, loc='best')\n",
    "plt.xlabel('Shadow size', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('4-qubit_randomclifford_depolar_p2=0.02.png', dpi=100, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23452caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4), dpi=100)\n",
    "plt.plot(shadow_range, errors['shadow'], '--o', ms=8, color='tab:orange', label='noisy')\n",
    "plt.plot(shadow_range, errors['shadow_r'], '--^', ms=8, color='tab:green', label='robust')\n",
    "plt.plot(shadow_range, errors['noiseless'], '--x', ms=8, color='tab:blue', label='noiseless')\n",
    "plt.plot(shadow_range, errors['prepcheck1'], '--o', ms=8, color='tab:red', label='prepcheck1')\n",
    "plt.plot(shadow_range, errors['prepcheck2'], '--o', ms=8, color='tab:purple', label='prepcheck2')\n",
    "plt.plot(shadow_range, errors['prepcheck3'], '--o', ms=8, color='tab:olive', label='prepcheck3')\n",
    "plt.plot(shadow_range, errors['prepcheck4'], '--o', ms=8, color='tab:pink', label='prepcheck4')\n",
    "plt.legend(fontsize=14, loc='best')\n",
    "plt.xlabel('Shadow size', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee15286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "# medians = [np.median(check, axis=2) for check in [expectations[f\"prepcheck{k+1}\"] for k in range(num_checks)]]\n",
    "# # medians = [np.median(check, axis=2) for check in [expectations[f\"check{k+1}\"] for k in range(num_checks)]]\n",
    "\n",
    "# shadow_size_index = -1  # largest shadow size\n",
    "# pauli_index = 6  # Example observable index\n",
    "# expectation_values = [median[shadow_size_index, pauli_index] for median in medians]\n",
    "\n",
    "# # Fit a Straight Line\n",
    "# check_numbers = [1, 2, 3]  # Numeric x-values for fitting\n",
    "# polynomial = Polynomial.fit(check_numbers, expectation_values, 1)\n",
    "\n",
    "# extrapolated_check = 4\n",
    "# extrapolated_value = polynomial(extrapolated_check)\n",
    "\n",
    "# Plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(check_numbers, expectation_values, color='blue', label='Measured Data')  # Only scatter plot for original data\n",
    "# plt.plot(np.linspace(1, 4, 4), polynomial(np.linspace(1, 4, 4)), color='red', label='Fitted Line')  # Keep the fitted line\n",
    "# plt.scatter([extrapolated_check], [extrapolated_value], color='green', label='Extrapolated for 4th Layer')\n",
    "\n",
    "# plt.xlabel('Number of Check Layers')\n",
    "# # plt.ylabel(f'Median Expectation Value for {shadow_range[shadow_size_index]} snapshots')\n",
    "# plt.ylabel('Expectation Value')\n",
    "# plt.title(f'Extrapolation of Expectation Value for Observable {Paulis[pauli_index]}')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.xticks(np.arange(1, 5))  # Set x-axis ticks in increments of 1\n",
    "# plt.savefig('extrapolation_ex.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811e263",
   "metadata": {},
   "source": [
    "Calculate Extrapolated Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_numbers = [1, 2, 3]  # Original check layers\n",
    "extrapolation_layers = range(4, 5)  # Layers we extrapolate to\n",
    "medians = [np.median(check, axis=2) for check in [expectations[f\"prepcheck{k+1}\"] for k in range(len(check_numbers))]]\n",
    "\n",
    "# Initialize a three-dimensional array to store extrapolated values\n",
    "# Dimensions: [extrapolated layers, shadow size, Paulis]\n",
    "expectation_check_limit = np.zeros((len(extrapolation_layers), len(shadow_range), len(Paulis)))\n",
    "\n",
    "for layer_index, layer in enumerate(extrapolation_layers):\n",
    "    for shadow_size_index in range(len(medians[0])):\n",
    "        for pauli_index in range(medians[0].shape[1]):\n",
    "            expectation_values = [median[shadow_size_index, pauli_index] for median in medians]\n",
    "            polynomial = Polynomial.fit(check_numbers, expectation_values, 1)\n",
    "            # Extrapolate the value for the current layer\n",
    "            extrapolated_value = polynomial(layer)\n",
    "            expectation_check_limit[layer_index, shadow_size_index, pauli_index] = extrapolated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834dcc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example Plotting for a specific Pauli index across all shadow sizes\n",
    "# pauli_index = 1  \n",
    "# shadow_size_index = -1 # largest shadow size\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# # Plotting the extrapolated values for each extrapolated layer for the specific observable and shadow size\n",
    "# for layer_index, layer in enumerate(extrapolation_layers):\n",
    "#     plt.plot(shadow_range, expectation_check_limit[layer_index, :, pauli_index], marker='o', linestyle='-', label=f'Extrapolated for {layer}th Layer')\n",
    "\n",
    "# plt.xlabel('Shadow Size')\n",
    "# plt.ylabel(f'Extrapolated Median Expectation Value')\n",
    "# plt.title(f'Extrapolated Values Across Shadow Sizes for Observable Index {pauli_index}')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda6568",
   "metadata": {},
   "source": [
    "Compute mean error across observables for each extrapolated check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_check_limit = np.zeros((len(extrapolation_layers), len(shadow_range)))\n",
    "\n",
    "for layer_index, layer in enumerate(extrapolation_layers):\n",
    "    for shadow_size_index in range(len(shadow_range)):\n",
    "        # Calculate the mean error for this layer and shadow size across all Pauli indices\n",
    "        error_check_limit[layer_index, shadow_size_index] = np.mean(\n",
    "            [np.abs(expectation_check_limit[layer_index, shadow_size_index, pauli_index] - ref_list[pauli_index]) for pauli_index in range(len(Paulis))]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed650d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4), dpi=100)\n",
    "plt.plot(shadow_range, errors['shadow'], '--o', ms=8, color='tab:orange', label='noisy')\n",
    "plt.plot(shadow_range, errors['prepcheck1'], '--o', ms=8, color='tab:red', label='prepcheck1')\n",
    "plt.plot(shadow_range, errors['prepcheck2'], '--o', ms=8, color='tab:purple', label='prepcheck2')\n",
    "plt.plot(shadow_range, errors['prepcheck3'], '--o', ms=8, color='tab:olive', label='prepcheck3')\n",
    "plt.plot(shadow_range, errors['prepcheck4'], '--o', ms=8, color='tab:pink', label='prepcheck4')\n",
    "plt.plot(shadow_range, errors['shadow_r'], '--^', ms=8, color='tab:green', label='robust')\n",
    "plt.plot(shadow_range, errors['noiseless'], '--x', ms=8, color='tab:blue', label='noiseless')\n",
    "\n",
    "# Plotting each layer of extrapolated checks\n",
    "colors = ['tab:brown', 'tab:gray', 'tab:cyan', 'tab:pink', 'tab:purple']  # Example colors for different layers\n",
    "for layer_index, layer in enumerate(extrapolation_layers):\n",
    "    plt.plot(shadow_range, error_check_limit[layer_index, :], '--o', ms=8, color=colors[layer_index % len(colors)], label=f'prepcheck {layer} (extrap)')\n",
    "\n",
    "# plt.legend(fontsize=14, loc='best')\n",
    "# plt.xlabel('Shadow size', fontsize=14)\n",
    "# plt.ylabel('Error', fontsize=14)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.tick_params(labelsize=14)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig('non_ideal_checks.png', dpi=100)\n",
    "# plt.show()\n",
    "\n",
    "# Adjust the legend to be outside without altering the figure size\n",
    "plt.legend(fontsize=10, loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "plt.xlabel('Shadow size', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "# Note: The figure's layout isn't altered with plt.tight_layout() in this case\n",
    "# Saving the figure with bbox_inches='tight' includes the external legend\n",
    "plt.savefig('4-qubit_stateprep_depolar_p2=0.02.png', dpi=100, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4), dpi=100)\n",
    "plt.plot(shadow_range, errors['shadow'], '--o', ms=8, color='tab:orange', label='noisy')\n",
    "plt.plot(shadow_range, errors['shadow_r'], '--^', ms=8, color='tab:green', label='robust')\n",
    "plt.plot(shadow_range, errors['noiseless'], '--x', ms=8, color='tab:blue', label='noiseless')\n",
    "plt.plot(shadow_range, errors['check1'], '--o', ms=8, color='tab:red', label='check1')\n",
    "plt.plot(shadow_range, errors['check2'], '--o', ms=8, color='tab:purple', label='check2')\n",
    "plt.plot(shadow_range, errors['check3'], '--o', ms=8, color='tab:olive', label='check3')\n",
    "plt.plot(shadow_range, errors['check4'], '--o', ms=8, color='tab:pink', label='check4')\n",
    "\n",
    "# Plotting each layer of extrapolated checks\n",
    "colors = ['tab:brown', 'tab:gray', 'tab:cyan', 'tab:pink', 'tab:purple']  # Example colors for different layers\n",
    "for layer_index, layer in enumerate(extrapolation_layers):\n",
    "    plt.plot(shadow_range, error_check_limit[layer_index, :], '--o', ms=8, color=colors[layer_index % len(colors)], label=f'check {layer} (extrap)')\n",
    "\n",
    "# Adjust the legend to be outside without altering the figure size\n",
    "plt.legend(fontsize=10, loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "plt.xlabel('Shadow size', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "plt.savefig('4-qubit_randomclifford_depolar_p2=0.02.png', dpi=100, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebcef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4c3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pcs_zne]",
   "language": "python",
   "name": "conda-env-pcs_zne-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
